{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Neural Networks\n",
    "\n",
    "<br>\n",
    "A single perceptron algorithm is a linear classifier that cannot successfully classify the outputs of an [XOR operation](https://en.wikipedia.org/wiki/Exclusive_or). With a multi-layer neural network consisting of hidden layer(s), we can construct a network beyond a linear clssifier. A simple multi-layer neural network can be broken down into a set of computational procedures, carried out in order and in cycles. Each step of the procedure has its special purpose, for clarity, we often write separate program functions for each of them. Below are some terminologies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Function \n",
    "\n",
    "Activation function defines the transformation of input to output at a single node. The activation function has to be nonlinear, otherwise the node can only carry out a linear transformation. Several popular activation functions are:\n",
    "* Sigmoid \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Hyperbolic tangent \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Rectified linear units (ReLu)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(z) = \n",
    "\\begin{cases}\n",
    "z  & if & z \\geq 0 \\\\\n",
    "0  & if & z < 0 \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid example\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        x: numerical value or array of values\n",
    "    Returns \n",
    "        output after sigmoid transformation\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cost Function \n",
    "\n",
    "Cost functionm, also called loss function or error function, describes the degree of error between model output and target output.  For all variations of ANN, defining a cost function is needed. With an objective of minimizing the cost function, ANN can learn to incrementally update the values of weights and achieve better predictive power. In ANN, whenever the network gives an output that is far from the target, we know we have to change our weights significantly for our next iteration. Some popular activation functions are:\n",
    "* Mean squared error \n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{N}\\sum_i (y_i - \\hat{y_i})^{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Cross entropy\n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = -\\sum_i p_i \\log \\hat{p_i}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_error(target, model_output):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        model output (array)\n",
    "        target (array)\n",
    "    Returns\n",
    "        error defined by cost function \n",
    "    \"\"\"\n",
    "    return np.sum((model_output - target)**2) / (2 * len(target))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation \n",
    "\n",
    "Forward propagation is the process of starting from input layer, multiply corresponding weights and go through transformations at each hidden layer, then arrive at the output layer. Between two adjacent layers we have calculations: \n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Z = XW + b \\\\\n",
    "A = \\sigma(Z) \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "where $X$ is input (or output of preceeding layer), $W$ the weights, and $\\sigma$ being the activation function at the nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, W):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        X (array): m X n matrix  \n",
    "                   m is number of observations, n is dimension of input  \n",
    "        W (array): n X p matrix \n",
    "                   n is number of nodes in preceeding layer, p is number of nodes in next layer\n",
    "    Returns\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Backward Propagation and Update Weights \n",
    "\n",
    "A lot of methods were tested to find an optimal set of weights. So far the best way for updating weights is to look for relationship between cost function $J$ and individual $w_i$. We can start by taking the derivative of $J$ with respect to $w_i$ and update its value depending on the error at each iteration. Each weight $w_i$ and bias $b$ can be updated as follow:\n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "w_i = w_i - \\alpha \\frac{\\partial J}{\\partial w_i} \\\\\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "with $\\alpha$ being learning rate, a hyperparameter which determines the amplitude of change at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def back_propagation(diff, A, X):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        diff: difference between layer_output and target\n",
    "        A: output of preceding layer \n",
    "        X: input of preceding layer\n",
    "    Returns\n",
    "        gradient of weights\n",
    "    \"\"\"\n",
    "    delta = diff * (A * (1.0 - A)) \n",
    "    gradient = np.dot(X.T, delta) \n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct Neural Network to Learn XOR Operation  \n",
    "\n",
    "Using the above individual pieces we can construct a simple 2-layer neural network (one output layer plus one hidden layer, input layer does not count). The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 3\n",
    "\n",
    "X_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # matrix with dimensions (4, 2)\n",
    "y_data = np.array([[0, 1, 1, 0]]).T  # dim (4, 1)\n",
    "\n",
    "W1 = 0.1 * np.random.randn(X.shape[1], num_nodes) # dim (2, 3)\n",
    "W2 = 0.1 * np.random.randn(num_nodes, 1) # dim (3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 10000 \n",
    "learning_rate = 1\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    # forward\n",
    "    a1 = forward_propagation(X_data, W1)\n",
    "    a2 = forward_propagation(a1, W2)\n",
    "\n",
    "    # back\n",
    "    d2 = (a2 - y_data[j]) \n",
    "    gradient2 = back_propagation(d2, a2, a1)\n",
    "\n",
    "    d1 = np.dot(d2, W2.T)\n",
    "    gradient1 = back_propagation(d1, a1, X_data)\n",
    "\n",
    "    # update weights\n",
    "    W2 -= learning_rate * gradient2\n",
    "    W1 -= learning_rate * gradient1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.25211819e-03],\n",
       "       [  1.15565982e-04],\n",
       "       [  1.15183323e-04],\n",
       "       [  7.13698486e-05]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "a1 = forward_propagation(X_data, W1)\n",
    "a2 = forward_propagation(a1, W2)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
