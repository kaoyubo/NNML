{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Neural Networks\n",
    "\n",
    "<br>\n",
    "A single perceptron algorithm is a linear classifier that cannot successfully classify the outputs of an [XOR operation](https://en.wikipedia.org/wiki/Exclusive_or). With a multi-layer neural network consisting of input, hidden layer(s), and output layer, we can construct a network beyond a linear clssifier. A simple multi-layer neural network can be broken down into a set of computational procedures, carried out in order and in cycles. Each step of the procedure has its special purpose, for clarity, we often write separate program functions for each of them. Below are some terminologies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Function \n",
    "\n",
    "Activation function defines the transformation of input to output at a single node. The activation function has to be nonlinear, otherwise the node can only carry out a linear transformation. Several popular activation functions are:\n",
    "* Sigmoid \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Hyperbolic tangent \n",
    "$$\n",
    "\\begin{equation*}\n",
    "tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Rectified linear units (ReLu)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(z) = \n",
    "\\begin{cases}\n",
    "z  & if & z \\geq 0 \\\\\n",
    "0  & if & z < 0 \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid example\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        x: numerical value or array of values\n",
    "    Returns \n",
    "        output after sigmoid transformation\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cost Function \n",
    "\n",
    "Cost functionm, also called loss function or error function, describes the degree of error between model output and target output.  For all variations of ANN, defining a cost function is needed. With an objective of minimizing the cost function, ANN can learn to incrementally update the values of weights and achieve better predictive power. In ANN, whenever the network gives an output that is far from the target, we know we have to change our weights significantly for our next iteration. Some popular activation functions are:\n",
    "* Mean squared error \n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{N}\\sum_i (y_i - \\hat{y_i})^{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Cross entropy\n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = -\\sum_i p_i \\log \\hat{p_i}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_error(target, model_output):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        model output (array)\n",
    "        target (array)\n",
    "    Returns\n",
    "        error defined by cost function \n",
    "    \"\"\"\n",
    "    return np.sum((model_output - target)**2) / (2 * len(target))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation \n",
    "\n",
    "Forward propagation is the process of starting from input layer, multiply corresponding weights and go through transformations at each hidden layer, then arrive at the output layer. Between two layers we have    \n",
    "$$\n",
    "\\begin{equation*}\n",
    "Z = WX + b \\\\\n",
    "a = \\sigma(Z) \n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(W, X, b):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        W (array): p X q matrix \n",
    "                   p is number of nodes in former layer, q is number of nodes for latter layer\n",
    "        X (array): m X n matrix  \n",
    "                   m is number of observations, n is dimension of input  \n",
    "    Returns\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(W, X.T) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Backward Propagation and Update Weights \n",
    "\n",
    "A lot of methods were tested to find an optimal set of weights. So far the best way for updating weights is to look for relationship between cost function $J$ and individual $w_i$. We can start by taking the derivative of $J$ with respect to $w_i$ and update its value depending on the error at each iteration. Each weight $w_i$ and bias $b$ can be updated as follow:\n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "w_i = w_i - \\alpha \\frac{\\partial J}{\\partial w_i} \\\\\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "with $\\alpha$ being learning rate, a hyperparameter which determines the amplitude of change at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct Neural Network to Learn XOR Operation  \n",
    "\n",
    "Using the above individual pieces we can construct a simple 2-layer neural network (one output layer plus one hidden layer, input layer does not count). The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # matrix with dimensions (4, 2)\n",
    "y = np.array([[0, 1, 1, 0]]).T  # dim (4, 1)\n",
    "\n",
    "\n",
    "W1 = [[0.1, -0.1], [0.1, -0.1]]\n",
    "b1 = 0.1\n",
    "W2 = [[-0.1, -0.1], [0.1, 0.1]]\n",
    "b2 = -0.1\n",
    "\n",
    "\n",
    "for i in range(len(X)):\n",
    "\n",
    "    a1 = forward_propagation(W1, X[i], b1)\n",
    "    a2 = forward_propagation(W2, a1, b2)\n",
    "    \n",
    "    e = calculate_error(y[i], a2)\n",
    "\n",
    "    e \n",
    "    \n",
    "    \n",
    "    # backward propagation\n",
    "    z2_delta = (z2 - y) * z2 * (1.0 - z2) \n",
    "    z2_gradient = np.dot(z1.T, z2_delta) \n",
    "    z1_delta = np.dot(z2_delta, W2.T) * z1 * (1.0 - z1) \n",
    "    z1_gradient = np.dot(X.T, z1_delta) \n",
    "    \n",
    "    # update weights\n",
    "    W2 -= learning_rate * z2_gradient\n",
    "    W1 -= learning_rate * z1_gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
