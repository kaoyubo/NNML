{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Neural Networks\n",
    "\n",
    "<br>\n",
    "A single perceptron algorithm is a linear classifier that cannot successfully classify the outputs of an [XOR logic gate](https://en.wikipedia.org/wiki/XOR_gate). In other words, we cannot find suitable numerical values for $w_1$, $w_2$, $b$ in euqation $y = w_1 x_1 + w_2 x_2$ + $b$ where $y$ and $X$ represent the outputs and inputs of XOR gate (see table). With a multi-layer neural network consisting of hidden layer(s), we can construct a network beyond a linear clssifier. A simple multi-layer neural network can be broken down into a set of computational procedures, carried out in order and in cycles. Each step of the procedure has its special purpose, for clarity, we often write separate program functions for each of them.  \n",
    "\n",
    "**XOR Logic Gate**\n",
    "\n",
    "|Input $x_1$|Input $x_2$|Output $y$|\n",
    "|---|---|---|\n",
    "|0  |0  |0  |\n",
    "|0  |1  |1  |\n",
    "|1  |0  |1  |\n",
    "|1  |1  |0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Function \n",
    "\n",
    "Activation function defines the transformation of input to output at a single node. The activation function has to be nonlinear, otherwise the node can only carry out a linear transformation. Several popular activation functions are:\n",
    "* Sigmoid \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Hyperbolic tangent \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Rectified linear units (ReLu)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(z) = \n",
    "\\begin{cases}\n",
    "z  & if & z \\geq 0 \\\\\n",
    "0  & if & z < 0 \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid example\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        x: numerical value or array of values\n",
    "    Returns \n",
    "        sigmoid transformation\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cost Function \n",
    "\n",
    "Cost functionm, also called loss function or error function, describes the degree of error between model output and target output.  For all variations of ANN, defining a cost function is needed. With the goal of minimizing cost function, ANN learns to iteratively update the values of weights and achieve better predictive power (for training data). Some popular activation functions are:\n",
    "* Mean squared error \n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{2N}\\sum_i (y_i - \\hat{y_i})^{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Cross entropy\n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = -\\sum_i p_i \\log \\hat{p_i}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example calculate MSE loss\n",
    "\n",
    "def calculate_error(target, model_output):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        model output (array)\n",
    "        target (array)\n",
    "    Returns\n",
    "        MSE loss  \n",
    "    \"\"\"\n",
    "    return np.sum((target - model_output)**2) / len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation \n",
    "\n",
    "Forward propagation is the process of input signals propagating forward through the network, and generate a model output at the last layer. For every layer in the network we have calculations: \n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Z = XW + b \\\\\n",
    "A = \\sigma(Z) \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "where $X$ is input for the layer, $W$ the weights, $Z$ the net input, $\\sigma$ the activation function, and $A$ the output (activation) of the layer. Sometimes we see net input decribed as $Z = W^{T}X + b$, and it is no different than the euqation above, just with matrices transposed and multiplication order reversed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, W):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        X (array): m X n matrix  \n",
    "                   m is number of observations, n is dimension of input  \n",
    "        W (array): n X p matrix \n",
    "                   n in is dimension of input, p is dimension of output\n",
    "    Returns\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Backward Propagation and Update Weights \n",
    "\n",
    "A lot of methods were tested to find an optimal set of weights. So far the best way for updating weights is to look for relationship between cost function $J$ and weight $w_{ij}$. We can start by taking the derivative of $J$ with respect to $w_{ij}$ and update its value depending on the derivative at each iteration. Each weight $w_{ij}$ and bias $b$ can be updated with \n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "w_{ij} = w_{ij} - \\alpha \\frac{\\partial J}{\\partial w_{ij}} \n",
    "\\end{equation*}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation*}\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b} \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "where $\\alpha$ is learning rate, a hyperparameter which determines the amplitude of change at each iteration. An example of $\\frac{\\partial J}{\\partial W}$ for MSE cost function and sigmoid activation function is:\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial W} & = \\frac{\\partial \\frac{1}{2} (y - A)^{2}}{\\partial Z} \\times \\frac{\\partial Z}{\\partial W} \\\\ \n",
    "& = (A - y) \\times \\sigma'(Z) \\times \\frac{\\partial Z}{\\partial W} \\\\\n",
    "& = (\\sigma(Z) - y) \\times \\sigma(Z) (1 - \\sigma(Z)) \\times X\n",
    "\\end{align*} \n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def back_propagation(diff, A, X):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        diff: difference between layer_output and target\n",
    "        A: layer output \n",
    "        X: layer input \n",
    "    Returns\n",
    "        gradient of weights\n",
    "    \"\"\"\n",
    "    delta = diff * (A * (1.0 - A)) \n",
    "    gradient = np.dot(X.T, delta) \n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct Neural Network to Learn XOR Operation  \n",
    "\n",
    "Using above pieces we can construct a simple 2-layer neural network (one output layer plus one hidden layer, input does not count as a layer). The goal is construct ANN which can represent an XOR operation. Mean squared error and sigmoid activation functions are used for both layers. There are two hyperparamters, one is num_nodes for the hidden layer, and another is learning_rate. In theory num_nodes needs to be at least 2 for training, but when num_nodes is small, efficiency of learning weights is sensitive to initialization. Sometimes weights learning can get stuck if the initial values are far from optimal solutions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 4\n",
    "\n",
    "X_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # matrix with dimensions (4, 2)\n",
    "y_data = np.array([[0], [1], [1], [0]])  # dim (4, 1)\n",
    "\n",
    "# initialize weights \n",
    "W1 = np.random.randn(X_data.shape[1], num_nodes) # dim (2, num_nodes)\n",
    "W2 = np.random.randn(num_nodes, 1) # dim (num_nodes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 100000 \n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # feed forward\n",
    "    a1 = forward_propagation(X_data, W1)\n",
    "    a2 = forward_propagation(a1, W2)\n",
    "\n",
    "    # backward propagation \n",
    "    d2 = (a2 - y_data) \n",
    "    gradient2 = back_propagation(d2, a2, a1)\n",
    "\n",
    "    d1 = np.dot(d2, W2.T)\n",
    "    gradient1 = back_propagation(d1, a1, X_data)\n",
    "\n",
    "    # update weights\n",
    "    W2 -= learning_rate * gradient2\n",
    "    W1 -= learning_rate * gradient1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01587092]\n",
      " [ 0.98718731]\n",
      " [ 0.98698926]\n",
      " [ 0.01007396]]\n"
     ]
    }
   ],
   "source": [
    "a1 = forward_propagation(X_data, W1)\n",
    "a2 = forward_propagation(a1, W2)\n",
    "\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
