{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Neural Networks\n",
    "\n",
    "<br>\n",
    "A single perceptron algorithm is a linear classifier that cannot successfully classify the outputs of an [XOR operation](https://en.wikipedia.org/wiki/Exclusive_or). With a multi-layer neural network consisting of hidden layer(s), we can construct a network beyond a linear clssifier. A simple multi-layer neural network can be broken down into a set of computational procedures, carried out in order and in cycles. Each step of the procedure has its special purpose, for clarity, we often write separate program functions for each of them. Below are some terminologies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Function \n",
    "\n",
    "Activation function defines the transformation of input to output at a single node. The activation function has to be nonlinear, otherwise the node can only carry out a linear transformation. Several popular activation functions are:\n",
    "* Sigmoid \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Hyperbolic tangent \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Rectified linear units (ReLu)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(z) = \n",
    "\\begin{cases}\n",
    "z  & if & z \\geq 0 \\\\\n",
    "0  & if & z < 0 \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid example\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        x: numerical value or array of values\n",
    "    Returns \n",
    "        sigmoid transformation\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cost Function \n",
    "\n",
    "Cost functionm, also called loss function or error function, describes the degree of error between model output and target output.  For all variations of ANN, defining a cost function is needed. With the goal of minimizing cost function, ANN learns to iteratively update the values of weights and achieve better predictive power (at least for training data). Some popular activation functions are:\n",
    "* Mean squared error \n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{N}\\sum_i (y_i - \\hat{y_i})^{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Cross entropy\n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = -\\sum_i p_i \\log \\hat{p_i}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example calculate MSE loss\n",
    "\n",
    "def calculate_error(target, model_output):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        model output (array)\n",
    "        target (array)\n",
    "    Returns\n",
    "        MSE loss  \n",
    "    \"\"\"\n",
    "    return np.sum((target - model_output)**2) / len(target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation \n",
    "\n",
    "Forward propagation is the process of input signals propagating forward through the network, and generate an output at the last layer. Between two adjacent layers we have calculations: \n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Z = XW + b \\\\\n",
    "A = \\sigma(Z) \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "where $X$ is input (or output of preceeding layer), $W$ the weights, and $\\sigma$ being the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, W):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        X (array): m X n matrix  \n",
    "                   m is number of observations, n is dimension of input  \n",
    "        W (array): n X p matrix \n",
    "                   n is number of nodes in preceeding layer, p is number of nodes in next layer\n",
    "    Returns\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Backward Propagation and Update Weights \n",
    "\n",
    "A lot of methods were tested to find an optimal set of weights. So far the best way for updating weights is to look for relationship between cost function $J$ and individual $w_i$. We can start by taking the derivative of $J$ with respect to $w_i$ and update its value depending on the error at each iteration. Each weight $w_i$ and bias $b$ can be updated as follow:\n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "w_i = w_i - \\alpha \\frac{\\partial J}{\\partial w_i} \\\\\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "with $\\alpha$ being learning rate, a hyperparameter which determines the amplitude of change at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def back_propagation(diff, A, X):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        diff: difference between layer_output and target\n",
    "        A: output of preceding layer \n",
    "        X: input of preceding layer\n",
    "    Returns\n",
    "        gradient of weights\n",
    "    \"\"\"\n",
    "    delta = diff * (A * (1.0 - A)) \n",
    "    gradient = np.dot(X.T, delta) \n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct Neural Network to Learn XOR Operation  \n",
    "\n",
    "Using the above individual pieces we can construct a simple 2-layer neural network (one output layer plus one hidden layer, input layer does not count). An ANN with input of 2 nodes, hidden layer of 3 nodes and output of one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_nodes = 3\n",
    "\n",
    "X_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # matrix with dimensions (4, 2)\n",
    "y_data = np.array([[0, 1, 1, 0]]).T  # dim (4, 1)\n",
    "\n",
    "W1 = 0.1 * np.random.randn(X_data.shape[1], num_nodes) # dim (2, 3)\n",
    "W2 = 0.1 * np.random.randn(num_nodes, 1) # dim (3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 10000 \n",
    "learning_rate = 1\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    # forward\n",
    "    a1 = forward_propagation(X_data, W1)\n",
    "    a2 = forward_propagation(a1, W2)\n",
    "\n",
    "    # back\n",
    "    d2 = (a2 - y_data) \n",
    "    gradient2 = back_propagation(d2, a2, a1)\n",
    "\n",
    "    d1 = np.dot(d2, W2.T)\n",
    "    gradient1 = back_propagation(d1, a1, X_data)\n",
    "\n",
    "    # update weights\n",
    "    W2 -= learning_rate * gradient2\n",
    "    W1 -= learning_rate * gradient1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03408027]\n",
      " [ 0.97995751]\n",
      " [ 0.97995751]\n",
      " [ 0.00142586]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = forward_propagation(X_data, W1)\n",
    "a2 = forward_propagation(a1, W2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
