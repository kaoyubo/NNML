{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Neural Networks\n",
    "\n",
    "<br>\n",
    "A single perceptron algorithm is a linear classifier that cannot successfully classify the outputs of an [XOR logic gate](https://en.wikipedia.org/wiki/XOR_gate). In other words, we cannot find suitable numerical values for $w_1$, $w_2$, $b$ in euqation $y = w_1 x_1 + w_2 x_2$ + $b$ where $y$ and $X$ represent the outputs and inputs of XOR gate (see table). With a multi-layer neural network consisting of hidden layer(s), we can construct a network beyond a linear clssifier. A simple multi-layer neural network can be broken down into a set of computational procedures, carried out in order and in cycles. Each step of the procedure has its special purpose, for clarity, we often write separate program functions for each of them.  \n",
    "\n",
    "**XOR Logic Gate**\n",
    "\n",
    "|Input $x_1$|Input $x_2$|Output $y$|\n",
    "|---|---|---|\n",
    "|0  |0  |0  |\n",
    "|0  |1  |1  |\n",
    "|1  |0  |1  |\n",
    "|1  |1  |0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activation Function \n",
    "\n",
    "Activation function defines the transformation of input to output at a single node. The activation function has to be nonlinear, otherwise the node can only carry out a linear transformation. Several popular activation functions are:\n",
    "* Sigmoid \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Hyperbolic tangent \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Rectified linear units (ReLu)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "f(z) = \n",
    "\\begin{cases}\n",
    "z  & if & z \\geq 0 \\\\\n",
    "0  & if & z < 0 \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid example\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        x: numerical value or array of values\n",
    "    Returns \n",
    "        sigmoid transformation\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cost Function \n",
    "\n",
    "Cost functionm, also called loss function or error function, describes the degree of error between model output and target output.  For all variations of ANN, defining a cost function is needed. With the goal of minimizing cost function, ANN learns to iteratively update the values of weights and achieve better predictive power (for training data). Some popular activation functions are:\n",
    "* Mean squared error \n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{2N}\\sum_i (y_i - \\hat{y_i})^{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* Cross entropy\n",
    "$$\n",
    "\\begin{equation*}\n",
    "J = -\\sum_i p_i \\log \\hat{p_i}\n",
    "\\end{equation*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example calculate MSE loss\n",
    "\n",
    "def calculate_error(target, model_output):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        model output (array)\n",
    "        target (array)\n",
    "    Returns\n",
    "        MSE loss  \n",
    "    \"\"\"\n",
    "    return np.sum((target - model_output)**2) / len(target)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation \n",
    "\n",
    "Forward propagation is the process of input signals propagating forward through the network, and generate a model output at the last layer. For every layer in the network we have calculations: \n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Z = XW + b \\\\\n",
    "A = \\sigma(Z) \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "where $X$ is input for the layer, $W$ the weights, $Z$ the net input, $\\sigma$ the activation function, and $A$ the output (also called activation) of the layer. Sometimes we see net input is decribed as $Z = W^{T}X + b$, and it is no different than the calculation above, just with matrices transposed and multiplication order reversed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, W):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        X (array): m X n matrix  \n",
    "                   m is number of observations, n is dimension of input  \n",
    "        W (array): n X p matrix \n",
    "                   n is number of nodes in preceeding layer, p is number of nodes in next layer\n",
    "    Returns\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. Backward Propagation and Update Weights \n",
    "\n",
    "A lot of methods were tested to find an optimal set of weights. So far the best way for updating weights is to look for relationship between cost function $J$ and weight $w_{ij}$. We can start by taking the derivative of $J$ with respect to $w_{ij}$ and update its value depending on the cost function at each iteration. Each weight $w_{ij}$ and bias $b$ can be updated as \n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "w_{ij} = w_{ij} - \\alpha \\frac{\\partial J}{\\partial w_{ij}} \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\begin{equation*}\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b} \n",
    "\\end{equation*}\n",
    "$$\n",
    "<br>\n",
    "where $\\alpha$ is learning rate, a hyperparameter which determines the amplitude of change at each iteration. An example of $\\frac{\\partial J}{\\partial W}$ for MSE cost function and sigmoid activation function is:\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial W} & = \\frac{\\partial \\frac{1}{2} (y - A)^{2}}{\\partial Z} \\times \\frac{\\partial Z}{\\partial W} \\\\ \n",
    "& = (A - y) \\times \\sigma'(Z) \\times \\frac{\\partial Z}{\\partial W} \\\\\n",
    "& = (\\sigma(Z) - y) \\times \\sigma(Z) (1 - \\sigma(Z)) \\times X\n",
    "\\end{align*} \n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def back_propagation(diff, A, X):\n",
    "    \"\"\"\n",
    "    Args \n",
    "        diff: difference between layer_output and target\n",
    "        A: output of preceding layer \n",
    "        X: input of preceding layer\n",
    "    Returns\n",
    "        gradient of weights\n",
    "    \"\"\"\n",
    "    delta = diff * (A * (1.0 - A)) \n",
    "    gradient = np.dot(X.T, delta) \n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct Neural Network to Learn XOR Operation  \n",
    "\n",
    "Using the above individual pieces we can construct a simple 2-layer neural network (one output layer plus one hidden layer, input does not count as a layer). For ANN to find solutions for a regression An ANN with 2D input, hidden layer of 3 nodes and output of one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.random.seed(0)\n",
    "num_nodes = 4\n",
    "\n",
    "X_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # matrix with dimensions (4, 2)\n",
    "y_data = np.array([[0, 1, 1, 0]]).T  # dim (4, 1)\n",
    "\n",
    "W1 = 0.1 * np.random.randn(X_data.shape[1], num_nodes) # dim (2, 3)\n",
    "W2 = 0.1 * np.random.randn(num_nodes, 1) # dim (3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 50000 \n",
    "learning_rate = 1\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    # forward\n",
    "    a1 = forward_propagation(X_data, W1)\n",
    "    a2 = forward_propagation(a1, W2)\n",
    "\n",
    "    # back\n",
    "    d2 = (a2 - y_data) \n",
    "    gradient2 = back_propagation(d2, a2, a1)\n",
    "\n",
    "    d1 = np.dot(d2, W2.T)\n",
    "    gradient1 = back_propagation(d1, a1, X_data)\n",
    "\n",
    "    # update weights\n",
    "    W2 -= learning_rate * gradient2\n",
    "    W1 -= learning_rate * gradient1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0092317 ]\n",
      " [ 0.99271026]\n",
      " [ 0.9920203 ]\n",
      " [ 0.00562092]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = forward_propagation(X_data, W1)\n",
    "a2 = forward_propagation(a1, W2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
