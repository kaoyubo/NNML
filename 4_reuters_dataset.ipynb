{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Newswire Dataset \n",
    "<br>\n",
    "A collection of newswire data is assembled for text classification purposes, and full description of the dataset can be found at [UCI machine learning repositoty](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection). Load data to jupyter notebook with Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  8982\n",
      "Number of test examples:  2246\n",
      "Example training data:  [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "Example training data label:  3\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters \n",
    "\n",
    "n = 10000  # top 10000 most common words\n",
    "\n",
    "(train_data, train_label), (test_data, test_label) = reuters.load_data(num_words=n)\n",
    "print('Number of training examples: ', train_data.shape[0])\n",
    "print('Number of test examples: ', test_data.shape[0])\n",
    "\n",
    "print('Example training data: ', train_data[0])\n",
    "print('Example training data label: ', train_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode Data to Newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_newswire(example):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            List of word indices \n",
    "        Returns:\n",
    "            List of words matched to given indices\n",
    "    \"\"\"\n",
    "    word_to_index = reuters.get_word_index()\n",
    "    index_to_word = {key: value for (value, key) in word_to_index.items()}\n",
    "    words = [index_to_word.get(i-3, 'UNK') for i in example] #indices offset by 3\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK UNK UNK said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print one example newswire\n",
    "decode_newswire(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exmples for each topic label:  Counter({3: 3159, 4: 1949, 19: 549, 16: 444, 1: 432, 11: 390, 20: 269, 13: 172, 8: 139, 10: 124, 9: 101, 21: 100, 25: 92, 2: 74, 18: 66, 24: 62, 0: 55, 34: 50, 12: 49, 36: 49, 28: 48, 6: 48, 30: 45, 23: 41, 31: 39, 17: 39, 40: 36, 32: 32, 41: 30, 14: 26, 26: 24, 39: 24, 43: 21, 15: 20, 38: 19, 37: 19, 29: 19, 45: 18, 5: 17, 7: 16, 27: 15, 22: 15, 42: 13, 44: 12, 33: 11, 35: 10})\n"
     ]
    }
   ],
   "source": [
    "print('Number of exmples for each topic label: ', Counter(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing \n",
    "\n",
    "All observations in traning dataset are lists of word indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Vectorized Input Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_vectors(X, N):\n",
    "    \"\"\"vectorize newswire data\"\"\"\n",
    "    input = np.zeros((X.shape[0], N))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(len(X[i])):\n",
    "            input[i][X[i][j]] = 1\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = construct_input_vectors(train_data, n)\n",
    "y_train = train_label\n",
    "X_test = construct_input_vectors(test_data, n)\n",
    "y_test = test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss \n",
    "For classification tasks, cross entropy loss funtion is often used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(X, W, b):\n",
    "    Z = np.maximum(np.dot(X, W) + b, 0) # element-wise max between two arrays\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(A):\n",
    "    exps = np.exp(A - np.max(A, axis=1, keepdims=True)) # prevent overflow\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(model_output, target):\n",
    "    ce = -np.sum(target * np.log(model_output) + (1 - target) * np.log(1 - model_output))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop_relu(delta, X):  \n",
    "    gradient = np.dot(X.T, delta)\n",
    "    gradient[X < 0] = 0\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y, X, W1, b1, W2, b2):\n",
    "    A1 = relu_activation(X, W1, b1)\n",
    "    class_prob = np.dot(A1, W2) + b2\n",
    "    pred = np.argmax(class_prob, axis=1)\n",
    "    print('prediction accuracy: %.2f%% \\n' % (100 * np.mean(pred == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 64 # size of hidden layer\n",
    "num_classes = 46 # number of classes\n",
    "batch_size = 32 #X_train.shape[0]\n",
    "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "learning_rate = 1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters \n",
    "np.random.seed(0)\n",
    "\n",
    "W1 = 0.01 * np.random.randn(n, h)\n",
    "b1 = np.zeros((1, h))\n",
    "W2 = 0.01 * np.random.randn(h, num_classes)\n",
    "b2 = np.zeros((1, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss 0.9684661784874473\n",
      "prediction accuracy: 72.44% \n",
      "\n",
      "epoch 1: loss 0.8889367327046472\n",
      "prediction accuracy: 80.71% \n",
      "\n",
      "epoch 2: loss 0.423098092830655\n",
      "prediction accuracy: 83.22% \n",
      "\n",
      "epoch 3: loss 0.44830396821304547\n",
      "prediction accuracy: 83.81% \n",
      "\n",
      "epoch 4: loss 0.3772117712930138\n",
      "prediction accuracy: 85.75% \n",
      "\n",
      "epoch 5: loss 0.5047915356294725\n",
      "prediction accuracy: 89.74% \n",
      "\n",
      "epoch 6: loss 0.38096821102222045\n",
      "prediction accuracy: 90.84% \n",
      "\n",
      "epoch 7: loss 0.3360112797655889\n",
      "prediction accuracy: 89.89% \n",
      "\n",
      "epoch 8: loss 0.35608725570136124\n",
      "prediction accuracy: 91.77% \n",
      "\n",
      "epoch 9: loss 0.3558634495363089\n",
      "prediction accuracy: 89.71% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# batch gradient descent \n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for j in range(num_batches):\n",
    "        \n",
    "        X_batch = X_train[j*batch_size : (j+1)*batch_size:, :]\n",
    "        y_batch = y_train[j*batch_size : (j+1)*batch_size:]\n",
    "        \n",
    "        # forward propogation\n",
    "        A1 = relu_activation(X_batch, W1, b1) \n",
    "        A2 = np.dot(A1, W2) + b2\n",
    "        probs = softmax(A2)  \n",
    "\n",
    "        # cross entropy loss \n",
    "        target_logprob = -np.log(probs[range(len(y_batch)), y_batch])\n",
    "        loss = np.sum(target_logprob) / batch_size\n",
    "\n",
    "        # backprop classification probs\n",
    "        d2 = probs\n",
    "        d2[range(len(y_batch)), y_batch] -= 1\n",
    "        d2 /= batch_size\n",
    "\n",
    "        dW2 = np.dot(A1.T, d2)\n",
    "        db2 = np.sum(d2, axis=0, keepdims=True)\n",
    "\n",
    "        # backprop into hidden layer\n",
    "        d1 = np.dot(d2, W2.T)\n",
    "        d1[A1 <= 0] = 0\n",
    "\n",
    "        dW1 = np.dot(X_batch.T, d1)\n",
    "        db1 = np.sum(d1, axis=0, keepdims=True)\n",
    "\n",
    "        # update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        \n",
    "    print(\"epoch {0}: loss {1}\".format(i, loss)) \n",
    "    # evaluate training set accuracy\n",
    "    evaluate_accuracy(y_train, X_train, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 73.86% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate test set accuracy\n",
    "evaluate_accuracy(y_test, X_test, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
